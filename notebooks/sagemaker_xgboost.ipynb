{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DF = \"CityGuardian_training_data__preview__regenerated.csv\"  # your 100-row file\n",
        "assert Path(TRAIN_DF).exists()\n",
        "\n",
        "df = pd.read_csv(TRAIN_DF, parse_dates=[\"timestamp\"])\n",
        "\n",
        "# --- make a tiny t+H target and a couple of simple rolling features (no leakage) ---\n",
        "HORIZON_STEPS = 2  # +2 minutes in your 1-min dataset\n",
        "\n",
        "df = df.sort_values([\"cell_id\",\"timestamp\"]).copy()\n",
        "df[\"utilization_t_plus\"] = df.groupby(\"cell_id\")[\"utilization\"].shift(-HORIZON_STEPS)\n",
        "\n",
        "grp = df.groupby(\"cell_id\")\n",
        "df[\"roll3_util_mean\"]  = grp[\"utilization\"].rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "df[\"roll3_util_slope\"] = df[\"roll3_util_mean\"] - grp[\"utilization\"].rolling(2, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "use = df[df[\"utilization_t_plus\"].notna()].copy()\n",
        "\n",
        "feature_cols = [\n",
        "    \"utilization\",\"neighbor_util_mean\",\n",
        "    \"p95_latency_ms\",\"call_success_rate\",\"handover_fail_rate\",\n",
        "    \"is_event\",\"event_profile\",\"row\",\"col\",\n",
        "    \"roll3_util_mean\",\"roll3_util_slope\",\n",
        "]\n",
        "X = use[feature_cols].fillna(0.0)\n",
        "y = use[\"utilization_t_plus\"].values\n",
        "\n",
        "# --- contiguous 'middle' test split so we catch the event block ---\n",
        "uniq_ts = np.array(sorted(use[\"timestamp\"].unique()))\n",
        "n = len(uniq_ts)\n",
        "tlen = max(2, int(np.ceil(n*0.25)))\n",
        "start = (n - tlen)//2; end = start + tlen\n",
        "ts_test = set(uniq_ts[start:end]); ts_train = set(uniq_ts[:start]); ts_val = set(uniq_ts[end:])\n",
        "\n",
        "i_train = use[\"timestamp\"].isin(ts_train).values\n",
        "i_val   = use[\"timestamp\"].isin(ts_val).values\n",
        "i_test  = use[\"timestamp\"].isin(ts_test).values\n",
        "\n",
        "X_train, y_train = X[i_train], y[i_train]\n",
        "X_val,   y_val   = X[i_val],   y[i_val]\n",
        "X_test          = X[i_test]\n",
        "y_test          = y[i_test]\n",
        "\n",
        "# --- write SageMaker-ready CSVs (label first, no header) ---\n",
        "import os\n",
        "os.makedirs(\"sm_data/train\", exist_ok=True)\n",
        "os.makedirs(\"sm_data/validation\", exist_ok=True)\n",
        "os.makedirs(\"sm_data/test\", exist_ok=True)\n",
        "\n",
        "pd.concat([pd.Series(y_train), X_train], axis=1).to_csv(\"sm_data/train/train.csv\", index=False, header=False)\n",
        "pd.concat([pd.Series(y_val),   X_val],   axis=1).to_csv(\"sm_data/validation/val.csv\", index=False, header=False)\n",
        "# features-only for prediction\n",
        "X_test.to_csv(\"sm_data/test/test_features.csv\", index=False, header=False)\n",
        "\n",
        "len(X_train), len(X_val), len(X_test), feature_cols\n"
      ],
      "metadata": {
        "id": "-GU_vJnFnHj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.s3 import S3Uploader  # tiny helper that doesn’t pull the full SDK\n",
        "# If import fails, replace with the boto3 upload below.\n",
        "\n",
        "S3_PREFIX = \"cityguardian/xgb-boto3\"\n",
        "s3_train_uri = S3Uploader.upload(\"sm_data/train\",      f\"s3://{S3_BUCKET}/{S3_PREFIX}/train\")\n",
        "s3_val_uri   = S3Uploader.upload(\"sm_data/validation\", f\"s3://{S3_BUCKET}/{S3_PREFIX}/validation\")\n",
        "s3_test_uri  = S3Uploader.upload(\"sm_data/test\",       f\"s3://{S3_BUCKET}/{S3_PREFIX}/test\")\n",
        "print(\"S3:\", s3_train_uri, s3_val_uri, s3_test_uri)\n"
      ],
      "metadata": {
        "id": "EC5etQ7unHhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, os\n",
        "s3 = boto3.client(\"s3\")\n",
        "def upload_dir(local_dir, bucket, prefix):\n",
        "    for root,_,files in os.walk(local_dir):\n",
        "        for f in files:\n",
        "            p = os.path.join(root,f)\n",
        "            key = f\"{prefix}/{os.path.relpath(p, local_dir)}\"\n",
        "            s3.upload_file(p, bucket, key)\n",
        "    return f\"s3://{bucket}/{prefix}\"\n",
        "\n",
        "S3_PREFIX = \"cityguardian/xgb-boto3\"\n",
        "s3_train_uri = upload_dir(\"sm_data/train\",      S3_BUCKET, f\"{S3_PREFIX}/train\")\n",
        "s3_val_uri   = upload_dir(\"sm_data/validation\", S3_BUCKET, f\"{S3_PREFIX}/validation\")\n",
        "s3_test_uri  = upload_dir(\"sm_data/test\",       S3_BUCKET, f\"{S3_PREFIX}/test\")\n",
        "print(\"S3:\", s3_train_uri, s3_val_uri, s3_test_uri)\n"
      ],
      "metadata": {
        "id": "5ZA9vuoMnHfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = boto3.client(\"sagemaker\", region_name=AWS_REGION)\n",
        "\n",
        "job_name = \"cityguardian-xgb-\" + datetime.datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "algo_spec = {\n",
        "    \"TrainingInputMode\": \"File\",\n",
        "    \"AlgorithmName\": \"xgboost\"   # built-in XGBoost container\n",
        "}\n",
        "\n",
        "hyperparams = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": \"rmse\",\n",
        "    \"num_round\": \"400\",\n",
        "    \"max_depth\": \"4\",\n",
        "    \"eta\": \"0.07\",\n",
        "    \"subsample\": \"0.9\",\n",
        "    \"colsample_bytree\": \"0.9\",\n",
        "    \"early_stopping_rounds\": \"20\"\n",
        "}\n",
        "\n",
        "input_data = [\n",
        "    {\n",
        "        \"ChannelName\":\"train\",\n",
        "        \"DataSource\":{\"S3DataSource\":{\n",
        "            \"S3DataType\":\"S3Prefix\",\n",
        "            \"S3Uri\": s3_train_uri,\n",
        "            \"S3DataDistributionType\":\"FullyReplicated\"\n",
        "        }},\n",
        "        \"ContentType\":\"text/csv\"\n",
        "    },\n",
        "    {\n",
        "        \"ChannelName\":\"validation\",\n",
        "        \"DataSource\":{\"S3DataSource\":{\n",
        "            \"S3DataType\":\"S3Prefix\",\n",
        "            \"S3Uri\": s3_val_uri,\n",
        "            \"S3DataDistributionType\":\"FullyReplicated\"\n",
        "        }},\n",
        "        \"ContentType\":\"text/csv\"\n",
        "    }\n",
        "]\n",
        "\n",
        "output_conf = {\"S3OutputPath\": f\"s3://{S3_BUCKET}/{S3_PREFIX}/output\"}\n",
        "\n",
        "resource_conf = {\"InstanceType\":\"ml.m5.large\", \"InstanceCount\":1, \"VolumeSizeInGB\":10}\n",
        "stop_conf = {\"MaxRuntimeInSeconds\": 3600}\n",
        "\n",
        "sm.create_training_job(\n",
        "    TrainingJobName=job_name,\n",
        "    AlgorithmSpecification=algo_spec,\n",
        "    RoleArn=ROLE_ARN,\n",
        "    InputDataConfig=input_data,\n",
        "    OutputDataConfig=output_conf,\n",
        "    ResourceConfig=resource_conf,\n",
        "    HyperParameters=hyperparams,\n",
        "    StoppingCondition=stop_conf,\n",
        ")\n",
        "\n",
        "def wait_training(name):\n",
        "    while True:\n",
        "        desc = sm.describe_training_job(TrainingJobName=name)\n",
        "        status = desc[\"TrainingJobStatus\"]\n",
        "        sec    = desc.get(\"SecondaryStatus\",\"\")\n",
        "        print(\"Status:\", status, sec)\n",
        "        if status in (\"Completed\",\"Failed\",\"Stopped\"):\n",
        "            break\n",
        "        time.sleep(30)\n",
        "    return desc\n",
        "\n",
        "desc = wait_training(job_name)\n",
        "model_artifact = desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
        "print(\"Model artifact:\", model_artifact)\n"
      ],
      "metadata": {
        "id": "0JHr0j9CnHc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = job_name + \"-model\"\n",
        "sm.create_model(\n",
        "    ModelName=model_name,\n",
        "    PrimaryContainer={\"Image\": XGB_IMAGE, \"ModelDataUrl\": model_artifact},\n",
        "    ExecutionRoleArn=ROLE_ARN\n",
        ")\n",
        "\n",
        "transform_job = job_name + \"-xform\"\n",
        "sm.create_transform_job(\n",
        "    TransformJobName=transform_job,\n",
        "    ModelName=model_name,\n",
        "    TransformInput={\n",
        "        \"DataSource\": {\"S3DataSource\": {\n",
        "            \"S3DataType\": \"S3Prefix\",\n",
        "            \"S3Uri\": s3_test_uri + \"/test_features.csv\"\n",
        "        }},\n",
        "        \"ContentType\":\"text/csv\",\n",
        "        \"SplitType\":\"Line\"\n",
        "    },\n",
        "    TransformOutput={\"S3OutputPath\": f\"s3://{S3_BUCKET}/{S3_PREFIX}/batch-preds\"},\n",
        "    TransformResources={\"InstanceType\":\"ml.m5.large\", \"InstanceCount\":1}\n",
        ")\n",
        "\n",
        "def wait_transform(name):\n",
        "    while True:\n",
        "        d = sm.describe_transform_job(TransformJobName=name)\n",
        "        status = d[\"TransformJobStatus\"]\n",
        "        print(\"Transform:\", status)\n",
        "        if status in (\"Completed\",\"Failed\",\"Stopped\"):\n",
        "            break\n",
        "        time.sleep(20)\n",
        "    return d\n",
        "\n",
        "tdesc = wait_transform(transform_job)\n",
        "preds_s3 = tdesc[\"TransformOutput\"][\"S3OutputPath\"]\n",
        "print(\"Batch preds at:\", preds_s3)\n"
      ],
      "metadata": {
        "id": "lSU7dloCnHal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the output file will be .../<job-name>/test_features.csv.out\n",
        "out_key_prefix = f\"{S3_PREFIX}/batch-preds/{transform_job}\"\n",
        "s3 = boto3.client(\"s3\")\n",
        "\n",
        "# list objects under that prefix\n",
        "resp = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=out_key_prefix)\n",
        "pred_key = [x[\"Key\"] for x in resp.get(\"Contents\", []) if x[\"Key\"].endswith(\".out\")][0]\n",
        "print(\"S3 key:\", pred_key)\n",
        "\n",
        "# download and load\n",
        "s3.download_file(S3_BUCKET, pred_key, \"preds.out\")\n",
        "y_pred = np.loadtxt(\"preds.out\", dtype=float)\n",
        "\n",
        "# metrics (same as before)\n",
        "from sklearn.metrics import mean_absolute_error, precision_recall_fscore_support, average_precision_score\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = float(np.sqrt(np.mean((y_test - y_pred)**2)))\n",
        "print(f\"SageMaker XGB: MAE={mae:.4f} RMSE={rmse:.4f}\")\n",
        "\n",
        "# alarm metrics (dynamic threshold from train set like before)\n",
        "thr = float(np.quantile(y_train, 0.80))\n",
        "yt = (y_test >= thr).astype(int); yp = (y_pred >= thr).astype(int)\n",
        "p,r,f1,_ = precision_recall_fscore_support(yt, yp, average=\"binary\", zero_division=0)\n",
        "ap = average_precision_score(yt, y_pred)\n",
        "print(f\"→ Alarm@{thr:.2f}: Precision={p:.3f} Recall={r:.3f} F1={f1:.3f} AP={ap:.3f}\")\n"
      ],
      "metadata": {
        "id": "XXrniiKZnHV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}